{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP2cvS73aCRZPMUK15XaqHT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lookinsight/ml/blob/main/20221113_ML%EC%A0%95%EB%A6%AC_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 머신러닝 정리"
      ],
      "metadata": {
        "id": "K-j1OUCQXLyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 선형회귀(Linear Regression) "
      ],
      "metadata": {
        "id": "JJ14dp4fXiND"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T3GP-nXXDsC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 데이터 수집\n",
        "\n",
        "file_name = \"insurance.csv\"   \n",
        "url = f'https://raw.githubusercontent.com/bigdata-young/bigdata_16th/main/data/{file_name}'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "df.head()\n",
        "df.info()\n",
        "df.describe()\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "\n",
        "# 데이터 전처리 (범주형)\n",
        "\n",
        "df.smoker.unique()\n",
        "df.smoker.eq('yes')\n",
        "# df.smoker.eq('yes') * 1\n",
        "df.smoker = df.smoker.eq('yes').mul(1)\n",
        "df.sex.unique()\n",
        "df.region.unique()\n",
        "df.region.nunique()\n",
        "df.dtypes\n",
        "\n",
        "df_dummy = pd.get_dummies(df, columns = ['sex','region'], drop_first = True)\n",
        "\n",
        "# 데이터 전처리\n",
        "\n",
        "df_dummy.columns\n",
        "X = df_dummy[['age', 'bmi', 'children', 'smoker', 'sex_male',\n",
        "       'region_northwest', 'region_southeast', 'region_southwest']]\n",
        "y = df_dummy.expenses\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 100)\n",
        "\n",
        "# 모델학습\n",
        "model.fit(X_train, y_train) \n",
        "\n",
        "# 예측\n",
        "pred = model.predict(X_test) \n",
        "\n",
        "# 모델 평가\n",
        "comparison = pd.DataFrame({'actual': y_test, 'pred': pred})\n",
        "\n",
        " # 평가위한 시각화\n",
        "plt.figure(figsize = (5, 5))\n",
        "sns.scatterplot(x = 'actual', y = 'pred', data = comparison)\n",
        "\n",
        "# MSE\n",
        "mean_squared_error(y_test, pred, squared = False) \n",
        "model.score(X_train, y_train) \n",
        "model.coef_\n",
        "pd.Series(model.coef_, index = X.columns) \n",
        "model.intercept_"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "267b5urAXKRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 로지스틱 회귀(Logistic Regression) "
      ],
      "metadata": {
        "id": "wUDLvXYdcYjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "\n",
        "# 데이터 불어오기\n",
        "file_url = \"https://raw.githubusercontent.com/bigdata-young/bigdata_16th/main/data/titanic_train.csv\"\n",
        "df_train = pd.read_csv(file_url, index_col = 0) \n",
        "\n",
        "df_train.head() \n",
        "df_train.info()\n",
        "df_train.describe(include=[\"O\"])\n",
        "# 범주형 컬럼들 확인\n",
        "df_train.Embarked.unique()\n",
        "# 상관관계\n",
        "df_train.corr()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.heatmap(df_train.corr(), cmap='coolwarm', vmin=-1, vmax=1, annot=True)\n",
        "\n",
        "# 결측치 처리 \n",
        "df_train.info()\n",
        "df_train.Embarked.value_counts()\n",
        "df_train['Embarked'].unique()\n",
        "df_train.Embarked = df_train['Embarked'].fillna('S')  \n",
        "df_train.Cabin.unique()\n",
        "df_train.drop(columns = ['Cabin'], inplace = True) \n",
        "df_train.drop(columns = ['Ticket'], inplace = True) \n",
        "df_train['Name'].dtype\n",
        "df_train.Name\n",
        "# df_train.Name.str.extract('([A-Za-z]+)\\.') # A-Z까지, a-z까지, +는 1개 이상, \\. 은 여러개\n",
        "df_train['Title'] = df_train.Name.str.extract('([A-Za-z]+)\\.')\n",
        "\n",
        "df_train.Title.value_counts()\n",
        "title_unique = df_train.Title.unique()\n",
        "\n",
        "rarelist = []\n",
        "for t in title_unique:\n",
        "    df_tlist = list(df_train.Title).count(t)\n",
        "    print(df_tlist)\n",
        "    if df_tlist < 10:\n",
        "        rarelist.append(t)\n",
        "\n",
        "\n",
        "df_train.Title = df_train['Title'].replace(rarelist, \"Rare\")\n",
        "title_age_mean = df_train.groupby(['Title'])['Age'].mean()\n",
        "\n",
        "for t in df_train.Title.unique():\n",
        "    df_train.loc[df_train['Age'].isnull() & (df_train.Title == t), 'Age'] = title_age_mean[t]\n",
        "\n",
        "df_train.drop(columns = ['Name', 'Title'], inplace = True) \n",
        "    \n",
        "df_train2 = pd.get_dummies(df_train, columns = ['Sex', 'Embarked'], drop_first = True)\n",
        "df_train2.info()\n",
        "\n",
        "# 모델링\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "model = LogisticRegression()\n",
        "X, y = (df_train2.drop(columns = ['Survived']), df_train2.Survived) \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 100) \n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "pred = model.predict(X_test)\n",
        "\n",
        "# 모델 평가\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(y_test, pred)\n",
        "model.coef_\n",
        "\n",
        "pd.options.display.float_format = '{:.4f}'.format\n",
        "pd.Series(model.coef_[0], index = X.columns) \n",
        "\n",
        "\n",
        "def pre_processing(df : pd.DataFrame):\n",
        "    df.Embarked = df.Embarked.fillna(\"S\") \n",
        "    df.Fare = df.Fare.fillna(0)\n",
        "    df['Title'] = df.Name.str.extract('([A-Za-z]+)\\.')\n",
        "    rarelist = [a for a in set(df['Title'])\n",
        "                if list(df['Title']).count(a) < 10]\n",
        "    df['Title'] = df['Title'].replace(rarelist, 'Rare') \n",
        "    title_age_mean = df.groupby(['Title'])['Age'].mean() \n",
        "    for v in df['Title'].unique():\n",
        "        df.loc[df.Age.isnull() & (df.Title == v), 'Age'] = title_age_mean[v]\n",
        "    df_clean = df.drop(columns=['Name', 'Ticket', 'Title', 'Cabin'])\n",
        "    return pd.get_dummies(df_clean,\n",
        "                          columns = ['Sex', 'Embarked'], drop_first=True)\n",
        "    \n",
        "file_url = \"https://raw.githubusercontent.com/bigdata-young/bigdata_16th/main/data/titanic_test.csv\"\n",
        "submission = pd.read_csv(f'{file_url}', index_col=0)\n",
        "\n",
        "# submission_df = pre_processing(submission)\n",
        "df_sub = pre_processing(submission)\n",
        "pred_sub = model.predict(df_sub)\n",
        "\n",
        "result = pd.DataFrame({'PassengerId':df_sub.index,'Survived':pred_sub})\n",
        "# result.to_csv('submission.csv')"
      ],
      "metadata": {
        "id": "pT4wGv7VXK_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0yR0QXK4XLCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. K-최근접이웃(K-nearest-neigbors)"
      ],
      "metadata": {
        "id": "KDjhOedaCBSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 데이터 불러오기\n",
        "file_url = 'https://raw.githubusercontent.com/bigdata-young/bigdata_16th/main/data/wine.csv'\n",
        "df = pd.read_csv(file_url)\n",
        "\n",
        "df.info()\n",
        "df.describe()\n",
        "\n",
        "# 출력되는 데이터의 소수점 4자리까지만 보기\n",
        "pd.options.display.float_format = '{:,.4f}'.format \n",
        "df.describe()\n",
        "\n",
        "df.Class.value_counts()\n",
        "value_counts = df.Class.value_counts()\n",
        "sns.barplot(x = value_counts.index, y = value_counts)\n",
        "\n",
        "# barplot 숫자표시  \n",
        "value_counts = df.Class.value_counts() \n",
        "bar = sns.barplot(x = value_counts.index, y = value_counts) \n",
        "for p in bar.patches:\n",
        "    height = p.get_height() \n",
        "    bar.text(p.get_x() + p.get_width() / 2., height + 3, height, ha = 'center', size = 9) \n",
        "bar.set_ylim(-5, 100) \n",
        "plt.show() \n",
        "\n",
        "# 스케일링\n",
        "# 표준화 스케일링\n",
        "# 로버스트 스케일링\n",
        "# 표준화 \n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "\n",
        "# 훈련셋과 시험셋 분리 \n",
        "from sklearn.model_selection import train_test_split \n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop('Class', axis = 1),    # axis = 0 행, axis = 1 축 \n",
        "                                                    df.Class, \n",
        "                                                    test_size = 0.2, random_state = 100)\n",
        "\n",
        "# 최대-최소 스케일러 사용 \n",
        "mm_scaler = MinMaxScaler() \n",
        "mm_scaler.fit(X_train)\n",
        "X_train_scaled = mm_scaler.transform(X_train) \n",
        "mm_scaler.fit(X_train)\n",
        "\n",
        "mm_scaler = MinMaxScaler()  # MinMaxScaler() 은 객체 , 객체는 안에 프로퍼티를 가지고 있음 \n",
        " # fit을 한다는건 특정한 데이터를 기준으로 학습을 시킴(프로퍼티라는 내부변수에 저장) \n",
        " # fit 하고 transform (한번에 fit_transform) 이게 가능한건 mm_scaler.fit 이 내부에 저장되어 있기 때문에 X_test는 transform 만 해줘도 됨\n",
        "X_train_scaled = mm_scaler.fit_transform(X_train)       \n",
        "X_test_scaled = mm_scaler.transform(X_test) \n",
        "X_test_scaled\n",
        "\n",
        "# 모델링\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier() \n",
        "knn.fit(X_train_scaled, y_train) \n",
        "pred = knn.predict(X_test_scaled) \n",
        "\n",
        "# 정확도보기 \n",
        "from sklearn.metrics import accuracy_score \n",
        "accuracy_score(y_test, pred) \n",
        "\n",
        "# 스케일링 안한 데이터 비교\n",
        "knn2 = KNeighborsClassifier() \n",
        "knn2.fit(X_train, y_train) \n",
        "pred2 = knn2.predict(X_test) \n",
        "accuracy_score(y_test, pred2) \n",
        "\n",
        "# 코드 진행 관련 함수화 하기 \n",
        "# n은 정수형 int 를 패러미터로 받고, 최종 return 으로 실수형 float 를 반환해준다.\n",
        "def tuning(n: int) -> float:\n",
        "    knn = KNeighborsClassifier(n_neighbors = n) \n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    pred = knn.predict(X_test_scaled)\n",
        "    score = accuracy_score(y_test, pred)  \n",
        "    return score \n",
        "\n",
        "# for 문을 통해서 1 ~ 20까지 tuning 값 구하기 \n",
        "for t in range(1, 21):\n",
        "    print(t, tuning(t)) \n",
        "\n",
        "k_list = [(t, tuning(t)) for t in range(1, 21)]\n",
        "\n",
        "sorted(k_list, key = lambda x: x[1], reverse = True) \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = [4, 5, 10, 4, 3, 11, 14 , 8, 10, 12]\n",
        "y = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\n",
        "classes = [0, 0, 1, 0, 0, 1, 1, 0, 1, 1]\n",
        "\n",
        "plt.scatter(x, y, c=classes)"
      ],
      "metadata": {
        "id": "Zz-GWuLZXLFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vsFHf6RPXLIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 나이브 베이즈(Naive Bayes) "
      ],
      "metadata": {
        "id": "BZfMZTHtDVs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "\n",
        "# 데이터 불러오기 \n",
        "file_url = 'https://raw.githubusercontent.com/bigdata-young/bigdata_16th/main/data/spam.csv'\n",
        "df = pd.read_csv(file_url, index_col = 0) \n",
        "df.head()\n",
        "df.info()\n",
        "df['target'].value_counts()\n",
        "df.text.nunique()\n",
        "df.text.unique()\n",
        "\n",
        "# 특수문자 제거\n",
        "import string\n",
        "string.punctuation\n",
        "\n",
        "# 첫번째 줄 샘플 데이터 보기\n",
        "first_text = df.text.loc[0] \n",
        "first_text\n",
        "\n",
        "for v in first_text:\n",
        "    if v not in string.punctuation:\n",
        "        print(v) \n",
        "\n",
        "'r' in 'warrior', 'k' in 'warrior', 'k' not in 'warrior' \n",
        "\n",
        "new_text = [] \n",
        "for v in first_text:\n",
        "    if v not in string.punctuation:\n",
        "        new_text.append(v) \n",
        "\n",
        "\"\".join(new_text) \n",
        "\n",
        "def remove_punc(text: str) -> str:\n",
        "    new_text = [c for c in text if c not in string.punctuation]\n",
        "    return \"\".join(new_text)\n",
        "\n",
        "remove_punc(df.text) \n",
        "\n",
        "df.text.apply(lambda text: \"\".join([c for c in text if c not in string.punctuation])) \n",
        "df.text = df['text'].apply(remove_punc) \n",
        "\n",
        "import nltk  # 자연어 처리 위한 세트\n",
        "nltk.download('stopwords') \n",
        "# corpus (말뭉치)\n",
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english') \n",
        "\n",
        "first_text = df.text.iloc[0]\n",
        "\n",
        "# 불용어 제거\n",
        "first_text.split() \n",
        "\n",
        "stop_eng = stopwords.words('english') \n",
        "\n",
        "# 불용어 확인\n",
        "for word in first_text.split():\n",
        "    if word in stop_eng:\n",
        "        print(word) \n",
        "\n",
        "# 불용어 아닌 단어 출력\n",
        "for word in first_text.split():\n",
        "    if word not in stop_eng:\n",
        "        print(word) \n",
        "\n",
        "# 불용어 아닌 단어 출력 - 소문자로 출력\n",
        "for word in first_text.split():\n",
        "    if word not in stop_eng:\n",
        "        print(word.lower())\n",
        "\n",
        "def remove_stop_words(text: str) -> str:\n",
        "    new_words = [] \n",
        "    for word in text.split():\n",
        "        if word not in stop_eng:\n",
        "            new_words.append(word.lower()) \n",
        "    return \" \".join(new_words) \n",
        "\n",
        "first_text, remove_stop_words(first_text)\n",
        "\n",
        "df.text = df.text.apply(remove_stop_words) \n",
        "df.target.unique() # 0, 1로\n",
        "df.target = df.target.map({'ham': 0, 'spam':1}) \n",
        "df.target.unique()\n",
        "\n",
        "# 카운트 기반으로 벡터화 하기\n",
        "df.text\n",
        "x = df.text  #독립변수 (series 1개 => 소문자)\n",
        "y = df.target   # 종속변수 (소문자)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer() # 클래스 -> 객체 -> cv \n",
        "cv.fit(x)\n",
        "cv.vocabulary_\n",
        "len(cv.vocabulary_)\n",
        "x = cv.transform(x) \n",
        "\n",
        "# 훈련셋 & 시험셋\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 100) \n",
        "\n",
        "# 모델 학습\n",
        "from sklearn.naive_bayes import MultinomialNB   \n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(x_train, y_train) \n",
        "pred = model.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "accuracy_score(y_test, pred)\n",
        "print(confusion_matrix(y_test, pred))\n",
        "\n",
        "# 가로 : 실제, 세로: 예측\n",
        "sns.heatmap(confusion_matrix(y_test, pred), cmap='coolwarm', annot=True, fmt='.0f')\n",
        "plt.title(\"Confusion Matrix :\")\n",
        "plt.ylabel('True')\n",
        "plt.xlabel('Predicted') \n",
        "plt.show() \n",
        "\n",
        "# 그래프 좀 더 세밀하게 표현\n",
        "cf_matrix = confusion_matrix(y_test,pred)\n",
        "group_names = ['TN','FP','FN','TP']\n",
        "print('group_names')\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n",
        "print('group_counts')\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "print('group_percentages')                     \n",
        "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "          zip(group_names,group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='coolwarm')\n",
        "plt.title(\"Confusion Matrix :\")\n",
        "plt.ylabel('True')\n",
        "plt.xlabel('Predicted') \n",
        "plt.show() "
      ],
      "metadata": {
        "id": "DGGay96FXLLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 결정트리(Desision Tree) "
      ],
      "metadata": {
        "id": "pyofz6oBF0r0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns \n",
        "\n",
        "# 데이터 불러오기\n",
        "\n",
        "file_url = 'https://raw.githubusercontent.com/bigdata-young/bigdata_16th/main/data/salary.csv'\n",
        "df = pd.read_csv(file_url, skipinitialspace = True) \n",
        "df.head()\n",
        "df.info()\n",
        "df.describe(include = ['O'])\n",
        "df.describe(include = 'all') \n",
        "df['capital-gain'].plot()\t\n",
        "\n",
        "# 전처리\n",
        "df['class']\n",
        "df['class'].value_counts()\n",
        "df['class'] = df['class'].map({'<=50K': 0, '>50K': 1})\n",
        "df['age'].dtype\n",
        "\n",
        "for c in df.columns:\n",
        "    print(c, df[c].dtype)\n",
        "\n",
        "obj_list = []\n",
        "for c in df.columns:\n",
        "    if df[c].dtype == 'object':\n",
        "        obj_list.append(c)\n",
        "        # print(c, df[c].dtype)\n",
        "print(obj_list)\n",
        "\n",
        "obj_list2 = [c for c in df.columns if df[c].dtype == 'object']\n",
        "print(obj_list2) \n",
        "\n",
        "for o in obj_list:\n",
        "    if df[o].nunique() > 10:\n",
        "        print(o,'-', df[o].nunique()) \n",
        "\n",
        "df.education.value_counts()\n",
        "df['education-num']\n",
        "\n",
        "for n in range(1, 17):\n",
        "    print(f\"**{n}**\", df[df['education-num'] == n]['education'].unique())\n",
        "\n",
        "df.drop(columns = ['education'], axis = 1, inplace = True)\n",
        "df.info()\n",
        "\n",
        "df['occupation'].value_counts()\n",
        "df['native-country'].value_counts()\n",
        "\n",
        "df.groupby(['native-country'])['class'].mean().sort_values(ascending = False) \n",
        "df[df['native-country'] == 'France'].groupby(['occupation'])['class'].mean() \n",
        "\n",
        "country_group = df.groupby('native-country').mean()['class']\n",
        "country_group.index\n",
        "country_group = country_group.reset_index()\n",
        "\n",
        "df = df.merge(country_group, on = 'native-country', how = 'left')\n",
        "df.drop('native-country', axis=1, inplace=True)\n",
        "df = df.rename(columns = {'class_x': 'class', 'class_y':'native-country'})\n",
        "\n",
        "# 결측치 처리 더미변수변환\n",
        "df.isna().mean()\n",
        "df['native-country'].fillna(-99, inplace=True)\n",
        "df['workclass'].value_counts() / len(df)\n",
        "df['workclass'].fillna('Private', inplace=True)\n",
        "df['occupation'].value_counts()\n",
        "df['occupation'].fillna('Unknown', inplace=True)\n",
        "df.info()\n",
        "df2 = pd.get_dummies(df, drop_first=True)\n",
        "df2.info()\n",
        "\n",
        "# 모델링 평가\n",
        "# 훈련셋 & 시험셋\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = df2.drop('class', axis=1)\n",
        "y = df2['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.4, random_state=100\n",
        ")\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = DecisionTreeClassifier(random_state = 100)\n",
        "model.fit(X_train, y_train)\n",
        "pred = model.predict(X_test)\n",
        "pred\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, pred)\n",
        "\n",
        "# 하이퍼 파라미터 튜닝\n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "\n",
        "model = DecisionTreeClassifier(random_state = 100)  \n",
        "model.fit(X_train, y_train)                      \n",
        "train_pred = model.predict(X_train)              \n",
        "test_pred = model.predict(X_test)                \n",
        "\n",
        "print('Train score :', accuracy_score(y_train, train_pred))\n",
        "print('Test score :', accuracy_score(y_test, test_pred))\n",
        "\n",
        "# 깊이를 제한했더니 학습이 덜 되면서 오히려 해로운 데이터에 대한 예측력 상승\n",
        "model = DecisionTreeClassifier(max_depth = 5, random_state = 100)  \n",
        "model.fit(X_train, y_train)                      \n",
        "train_pred = model.predict(X_train)              \n",
        "test_pred = model.predict(X_test)                \n",
        "print('Train score :', accuracy_score(y_train, train_pred) )\n",
        "print('Test score :', accuracy_score(y_test, test_pred))\n",
        "\n",
        "# max_depth : 7\n",
        "model = DecisionTreeClassifier(max_depth = 7, random_state = 100)  \n",
        "model.fit(X_train, y_train)                      \n",
        "train_pred = model.predict(X_train)              \n",
        "test_pred = model.predict(X_test)                \n",
        "print('Train score :', accuracy_score(y_train, train_pred) )\n",
        "print('Test score :', accuracy_score(y_test, test_pred))\n",
        "\n",
        "# 함수화\n",
        "\n",
        "def test_depth(depth: int):\n",
        "    model = DecisionTreeClassifier(max_depth = depth, random_state = 100)  \n",
        "    model.fit(X_train, y_train)                      \n",
        "    train_pred = model.predict(X_train)              \n",
        "    test_pred = model.predict(X_test)                \n",
        "    print(f'**{depth}**')\n",
        "    print('Train score :', accuracy_score(y_train, train_pred) )\n",
        "    print('Test score :', accuracy_score(y_test, test_pred))\n",
        "\n",
        "# [test_depth(d) for d in range(1, 20)]\n",
        "for d in range(1, 21):\n",
        "    test_depth(d)\n",
        "\n",
        "# 트리그래프\n",
        "from sklearn.tree import plot_tree \n",
        "plt.figure(figsize=(30,10))  # 그래프 크기 설정\n",
        "plot_tree(model) \n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(30,10))  # 그래프 크기 설정\n",
        "plot_tree(model, max_depth=3, fontsize = 15)\n",
        "plt.show()\n",
        "\n",
        "# feature name 표시\n",
        "plt.figure(figsize=(30,10))  # 그래프 크기 설정\n",
        "plot_tree(model, max_depth=3, fontsize = 15, feature_names = X_train.columns)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6E5fpop7XLOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MwxnLEMQXLUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l9XTPvtvXLX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "poWnWGQPXLat"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}